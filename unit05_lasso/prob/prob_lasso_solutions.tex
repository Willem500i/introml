\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{amsmath, amssymb, bm, cite, epsfig, psfrag}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{enumerate}
\usepackage{listings}
\usepackage{mathtools}
\lstloadlanguages{Python}
\usetikzlibrary{shapes,arrows}

\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{9} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{9}  % for normal
% Defining colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\def\del{\partial}
\def\ds{\displaystyle}
\def\ts{\textstyle}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\beqa{\begin{eqnarray}}
\def\eeqa{\end{eqnarray}}
\def\beqan{\begin{eqnarray*}}
\def\eeqan{\end{eqnarray*}}
\def\nn{\nonumber}
\def\binomial{\mathop{\mathrm{binomial}}}
\def\half{{\ts\frac{1}{2}}}
\def\Half{{\frac{1}{2}}}
\def\N{{\mathbb{N}}}
\def\Z{{\mathbb{Z}}}
\def\Q{{\mathbb{Q}}}
\def\R{{\mathbb{R}}}
\def\C{{\mathbb{C}}}
\def\argmin{\mathop{\mathrm{arg\,min}}}
\def\argmax{\mathop{\mathrm{arg\,max}}}
\def\diag{\mathop{\mathrm{diag}}}
\def\x{\times}
\def\limn{\lim_{n \rightarrow \infty}}
\def\liminfn{\liminf_{n \rightarrow \infty}}
\def\limsupn{\limsup_{n \rightarrow \infty}}
\def\GV{Guo and Verd{\'u}}
\def\MID{\,|\,}
\def\MIDD{\,;\,}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{claim}{Claim}
\def\qed{\mbox{} \hfill $\Box$}
\setlength{\unitlength}{1mm}

\def\bhat{\widehat{b}}
\def\ehat{\widehat{e}}
\def\phat{\widehat{p}}
\def\qhat{\widehat{q}}
\def\rhat{\widehat{r}}
\def\shat{\widehat{s}}
\def\uhat{\widehat{u}}
\def\ubar{\overline{u}}
\def\vhat{\widehat{v}}
\def\xhat{\widehat{x}}
\def\xbar{\overline{x}}
\def\zhat{\widehat{z}}
\def\zbar{\overline{z}}
\def\la{\leftarrow}
\def\ra{\rightarrow}
\def\MSE{\mbox{\small \sffamily MSE}}
\def\SNR{\mbox{\small \sffamily SNR}}
\def\SINR{\mbox{\small \sffamily SINR}}
\def\arr{\rightarrow}
\def\Exp{\mathbb{E}}
\def\var{\mbox{var}}
\def\Tr{\mbox{Tr}}
\def\tm1{t\! - \! 1}
\def\tp1{t\! + \! 1}

\def\Xset{{\cal X}}

\newcommand{\one}{\mathbf{1}}
\newcommand{\abf}{\mathbf{a}}
\newcommand{\bbf}{\mathbf{b}}
\newcommand{\dbf}{\mathbf{d}}
\newcommand{\ebf}{\mathbf{e}}
\newcommand{\gbf}{\mathbf{g}}
\newcommand{\hbf}{\mathbf{h}}
\newcommand{\pbf}{\mathbf{p}}
\newcommand{\pbfhat}{\widehat{\mathbf{p}}}
\newcommand{\qbf}{\mathbf{q}}
\newcommand{\qbfhat}{\widehat{\mathbf{q}}}
\newcommand{\rbf}{\mathbf{r}}
\newcommand{\rbfhat}{\widehat{\mathbf{r}}}
\newcommand{\sbf}{\mathbf{s}}
\newcommand{\sbfhat}{\widehat{\mathbf{s}}}
\newcommand{\ubf}{\mathbf{u}}
\newcommand{\ubfhat}{\widehat{\mathbf{u}}}
\newcommand{\utildebf}{\tilde{\mathbf{u}}}
\newcommand{\vbf}{\mathbf{v}}
\newcommand{\vbfhat}{\widehat{\mathbf{v}}}
\newcommand{\wbf}{\mathbf{w}}
\newcommand{\wbfhat}{\widehat{\mathbf{w}}}
\newcommand{\xbf}{\mathbf{x}}
\newcommand{\xbfhat}{\widehat{\mathbf{x}}}
\newcommand{\xbfbar}{\overline{\mathbf{x}}}
\newcommand{\ybf}{\mathbf{y}}
\newcommand{\zbf}{\mathbf{z}}
\newcommand{\zbfbar}{\overline{\mathbf{z}}}
\newcommand{\zbfhat}{\widehat{\mathbf{z}}}
\newcommand{\Ahat}{\widehat{A}}
\newcommand{\Abf}{\mathbf{A}}
\newcommand{\Bbf}{\mathbf{B}}
\newcommand{\Cbf}{\mathbf{C}}
\newcommand{\Bbfhat}{\widehat{\mathbf{B}}}
\newcommand{\Dbf}{\mathbf{D}}
\newcommand{\Gbf}{\mathbf{G}}
\newcommand{\Hbf}{\mathbf{H}}
\newcommand{\Ibf}{\mathbf{I}}
\newcommand{\Kbf}{\mathbf{K}}
\newcommand{\Pbf}{\mathbf{P}}
\newcommand{\Phat}{\widehat{P}}
\newcommand{\Qbf}{\mathbf{Q}}
\newcommand{\Rbf}{\mathbf{R}}
\newcommand{\Rhat}{\widehat{R}}
\newcommand{\Sbf}{\mathbf{S}}
\newcommand{\Ubf}{\mathbf{U}}
\newcommand{\Vbf}{\mathbf{V}}
\newcommand{\Wbf}{\mathbf{W}}
\newcommand{\Xhat}{\widehat{X}}
\newcommand{\Xbf}{\mathbf{X}}
\newcommand{\Ybf}{\mathbf{Y}}
\newcommand{\Zbf}{\mathbf{Z}}
\newcommand{\Zhat}{\widehat{Z}}
\newcommand{\Zbfhat}{\widehat{\mathbf{Z}}}
\def\alphabf{{\boldsymbol \alpha}}
\def\betahat{\widehat{\beta}}
\def\betabf{{\boldsymbol \beta}}
\def\betabfhat{{\widehat{\bm{\beta}}}}
\def\epsilonbf{{\boldsymbol \epsilon}}
\def\mubf{{\boldsymbol \mu}}
\def\lambdabf{{\boldsymbol \lambda}}

\def\betabf{{\boldsymbol \beta}}

\def\etabf{{\boldsymbol \eta}}
\def\xibf{{\boldsymbol \xi}}
\def\taubf{{\boldsymbol \tau}}
\def\sigmahat{{\widehat{\sigma}}}
\def\thetabf{{\bm{\theta}}}
\def\thetabfhat{{\widehat{\bm{\theta}}}}
\def\thetahat{{\widehat{\theta}}}
\def\mubar{\overline{\mu}}
\def\muavg{\mu}
\def\sigbf{\bm{\sigma}}
\def\etal{\emph{et al.}}
\def\Ggothic{\mathfrak{G}}
\def\Pset{{\mathcal P}}
\newcommand{\bigCond}[2]{\bigl({#1} \!\bigm\vert\! {#2} \bigr)}
\newcommand{\BigCond}[2]{\Bigl({#1} \!\Bigm\vert\! {#2} \Bigr)}
\newcommand{\tran}{^{\text{\sf T}}}
\newcommand{\herm}{^{\text{\sf H}}}
\newcommand{\bkt}[1]{{\langle #1 \rangle}}
\def\Norm{{\mathcal N}}
\newcommand{\vmult}{.}
\newcommand{\vdiv}{./}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
backgroundcolor=\color{backcolour},
commentstyle=\color{deepgreen},
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
%frame=tb,                         % Any extra options here
showstringspaces=false            %
}}

% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pycode[1]{{\pythonstyle\lstinline!#1!}}

\begin{document}

\title{Introduction to Machine Learning\\
Problems:  LASSO and Model Selection\\
Solutions}
\author{Willem Neuefeind Lessig\\
Completed: October 1, 2025}
\date{}

\maketitle

\begin{enumerate}

\item \emph{Exhaustive search.}  In this problem, we will look at how to exhaustively search
over all possible subsets of features.  You are given three python functions:
\begin{python}
    model = LinearRegression()  # Create a linear regression model object
    model.fit(X,y)              # Fits the model
    yhat = model.predict(X)     # Predicts targets given features
\end{python}
Given training data \pycode{Xtr,ytr} and test data \pycode{Xts,yts},
write a few lines of python code to:
\begin{enumerate}[(a)]
\item Find the best model using only one feature of the data (i.e.\ one column of
\pycode{Xtr} and \pycode{Xts}).

\textbf{Solution:}
\begin{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

best_mse = float('inf')
best_feature = None

for i in range(Xtr.shape[1]):
    model = LinearRegression()
    model.fit(Xtr[:, i:i+1], ytr)
    yhat = model.predict(Xts[:, i:i+1])
    mse = mean_squared_error(yts, yhat)
    
    if mse < best_mse:
        best_mse = mse
        best_feature = i

print(f"Best single feature: {best_feature}, MSE: {best_mse}")
\end{python}

\item Find the best model using only two features of the data (i.e.\ two columns of
\pycode{Xtr} and \pycode{Xts}).

\textbf{Solution:}
\begin{python}
from itertools import combinations

best_mse = float('inf')
best_features = None

for feature_pair in combinations(range(Xtr.shape[1]), 2):
    model = LinearRegression()
    model.fit(Xtr[:, feature_pair], ytr)
    yhat = model.predict(Xts[:, feature_pair])
    mse = mean_squared_error(yts, yhat)
    
    if mse < best_mse:
        best_mse = mse
        best_features = feature_pair

print(f"Best two features: {best_features}, MSE: {best_mse}")
\end{python}

\item Suppose we wish to find the best $k$ of $p$ features via exhaustive searching over all
possible subsets of features.  How many times would you need to call the \pycode{fit} function?
What if $k=10$ and $p=1000$?

\textbf{Solution:}
The number of times we need to call the \pycode{fit} function is $\binom{p}{k} = \frac{p!}{k!(p-k)!}$.

For $k=10$ and $p=1000$:
\[
\binom{1000}{10} = \frac{1000!}{10! \cdot 990!} \approx 2.63 \times 10^{23}
\]

This is computationally infeasible, which is why we need regularization methods like LASSO.

\end{enumerate}


\item \emph{Selecting a regularizer.}  Suppose we fit a regularized least squares objective,
\[
    J(\wbf) = \sum_{i=1}^N (y_i - \hat{y}_i)^2 + \lambda\phi(\wbf),
\]
where $\hat{y}_i$ is some prediction of $y_i$ given the model parameters $\wbf$.
For each case below, suggest a possible regularization function $\phi(\wbf)$.
There is no single correct answer.

\textbf{Solutions:}
\begin{enumerate}[(a)]
\item All parameters vectors $\wbf$ should be considered.

$\phi(\wbf) = 0$ (no regularization)

\item Negative values of $w_j$ are unlikely (but still possible).

$\phi(\wbf) = \sum_{j} \max(0, -w_j)$ or $\phi(\wbf) = \sum_{j} e^{-w_j}$

\item For each $j$, $w_j$ should not change that significantly from $w_{j-1}$.

$\phi(\wbf) = \sum_{j=2}^p (w_j - w_{j-1})^2$ (total variation regularization)

\item For most $j$, $w_j=w_{j-1}$.  However, it can happen that $w_j$ can be different from $w_{j-1}$
for a few indices $j$.

$\phi(\wbf) = \sum_{j=2}^p |w_j - w_{j-1}|$ (fused LASSO)
\end{enumerate}

\item \label{prob:house_price}
\emph{Normalization.}  A data analyst for a real estate firm wants to predict house prices based on
two features in each zip code.  The features are shown in Table~\ref{tbl:house_features}.
The agent decides to use a linear model,
\beq \label{eq:yunnorm}
    \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2, 
\eeq

\begin{table}
\centering
\begin{tabular}{|l|l|l|l|}
\hline
Variable & Units & Mean  & Std dev  \\ \hline
Median income, $x_1$ & \$ & 50000 & 15000 \\ \hline
Median age, $x_2$ & years & 45 & 10 \\ \hline
House sale price, $y$ & \$1000 & 300 & 100 \\ \hline
\end{tabular}
\caption{Features for Problem~\ref{prob:house_price}} \label{tbl:house_features}
\end{table}

\begin{enumerate}[(a)]
\item What is the problem in using a LASSO regularizer of the form,
\[
    \phi(\betabf) = \sum_{j=1}^2 |\beta_j|.
\]

\textbf{Solution:}
The problem is that the features have very different scales. Income is measured in dollars (mean 50,000) while age is measured in years (mean 45). This means $\beta_1$ will be much smaller than $\beta_2$ to achieve similar impact on the prediction. The LASSO penalty $|\beta_1| + |\beta_2|$ will unfairly penalize the coefficient for income more heavily, even though both features might be equally important.

\item To uniformly regularize the features, she fits a model on the normalized features,
\[
    \hat{u} = \alpha_1 z_1 + \alpha_2 z_2, \quad z_j = \frac{x_j - \bar{x}_j}{s_j},
    \quad u = \frac{\hat{y}-\bar{y}}{s_y},
\]
where $s_j$ and $s_y$ are the standard deviations of the  $x_{j}$ and $y$.
She obtains parameters $\alphabf = [0.6,-0.3]$?  What are the parameters $\beta$ in the original model
\eqref{eq:yunnorm}?

\textbf{Solution:}
We need to transform back from the normalized model to the original model.

Starting with: $\hat{u} = \alpha_1 z_1 + \alpha_2 z_2$

Substituting the definitions:
\[
\frac{\hat{y} - \bar{y}}{s_y} = \alpha_1 \frac{x_1 - \bar{x}_1}{s_1} + \alpha_2 \frac{x_2 - \bar{x}_2}{s_2}
\]

Solving for $\hat{y}$:
\[
\hat{y} = \bar{y} + s_y \left( \alpha_1 \frac{x_1 - \bar{x}_1}{s_1} + \alpha_2 \frac{x_2 - \bar{x}_2}{s_2} \right)
\]

\[
\hat{y} = \bar{y} - s_y \left( \alpha_1 \frac{\bar{x}_1}{s_1} + \alpha_2 \frac{\bar{x}_2}{s_2} \right) + s_y \frac{\alpha_1}{s_1} x_1 + s_y \frac{\alpha_2}{s_2} x_2
\]

Therefore:
\begin{align}
\beta_0 &= \bar{y} - s_y \left( \alpha_1 \frac{\bar{x}_1}{s_1} + \alpha_2 \frac{\bar{x}_2}{s_2} \right)\\
\beta_1 &= s_y \frac{\alpha_1}{s_1} = 100 \times \frac{0.6}{15000} = 0.004\\
\beta_2 &= s_y \frac{\alpha_2}{s_2} = 100 \times \frac{-0.3}{10} = -3
\end{align}

\[
\beta_0 = 300 - 100 \left( 0.6 \times \frac{50000}{15000} + (-0.3) \times \frac{45}{10} \right) = 300 - 100(2 - 1.35) = 235
\]

So $\betabf = [235, 0.004, -3]$.
\end{enumerate}


\item \emph{Normalization in python.}  You are given python functions,
\begin{python}
    model = SomeModel()         # Creates a model
    model.fit(Z,u)              # Fits the model, expecting normalized features
    yhat = model.predict(Z)     # Predicts targets given features
\end{python}
Given training data \pycode{Xtr,ytr} and test data \pycode{Xts,yts},
write python code to:
\begin{itemize}
\item Normalize the training data to remove the mean and standard deviation from both
\pycode{Xtr} and \pycode{ytr}.
\item Fit the model on the normalized data.
\item Predict the values \pycode{yhat} on the test data.
\item Measure the RSS on the test data.
\end{itemize}

\textbf{Solution:}
\begin{python}
import numpy as np

# Normalize training data
Xtr_mean = np.mean(Xtr, axis=0)
Xtr_std = np.std(Xtr, axis=0)
ytr_mean = np.mean(ytr)
ytr_std = np.std(ytr)

Ztr = (Xtr - Xtr_mean) / Xtr_std
utr = (ytr - ytr_mean) / ytr_std

# Fit the model on normalized data
model = SomeModel()
model.fit(Ztr, utr)

# Normalize test data using training statistics
Zts = (Xts - Xtr_mean) / Xtr_std

# Predict on normalized test data
u_pred = model.predict(Zts)

# Transform predictions back to original scale
yhat = u_pred * ytr_std + ytr_mean

# Measure RSS on test data
rss = np.sum((yts - yhat)**2)
print(f"RSS on test data: {rss}")
\end{python}


\item \emph{Discretization.}  Suppose we wish to fit a model,
\beq \label{eq:ynl}
    y \approx \hat{y} = \sum_{j=1}^K \beta_j e^{-\alpha_j x},
\eeq
for parameters $\alpha_j$ and $\beta_j$.  Since the parameters $\alpha_j$ are not known,
this model is nonlinear and cannot be fit with least squares.
A common approach in such circumstances is to use an alternate linear model,
\beq \label{eq:ydis}
    y \approx \hat{y} = \sum_{j=1}^p \tilde{\beta}_j e^{-\tilde{\alpha}_j x},
\eeq
where the values $\tilde{\alpha}_1,\ldots,\tilde{\alpha}_p$ are a \emph{fixed}, 
large set of possible values for $\alpha_j$,
and $\tilde{\beta}_j$ are the coefficients in the model.  Since the values $\tilde{\alpha}_j$
are fixed, only the parameters $\tilde{\beta}_j$ need to be learned.
Hence, the model \eqref{eq:ydis} is linear.  The model \eqref{eq:ydis}
is equivalent to \eqref{eq:ynl} if only a small number $K$ of the coefficients $\tilde{\beta}_j$ are
non-zero.
You are given three python functions:
\begin{python}
    model = Lasso(lam=lam)           # Creates a linear LASSO model
                                     # with a regularization lam
    beta = model.fit(Z,y)            # Finds the model parameters using the
                                     # LASSO objective
                                     #  ||y-Z*beta||^2 + lam*||beta||_1
    yhat = model.predict(Z)          # Predicts targets given features Z:
                                     #   yhat = Z*beta
\end{python}
Note this syntax is slightly different from the \pycode{sklearn} syntax.
You are also given training data \pycode{xtr,ytr} and test data \pycode{xts,yts}.
Write python code to:
\begin{itemize}
\item Create $p=100$ values of $\tilde{\alpha}_j$ uniformly in some interval $\tilde{\alpha}_j \in [a,b]$
where $a$ and $b$ are given.
\item Fit the linear model \eqref{eq:ydis} on the training data for some given \pycode{lam}.
\item Measure the test error.
\item Find coefficients $\alpha_j$ and $\beta_j$ corresponding to the largest $k=3$ values
in $\tilde{\beta}_j$.  You can use the function \pycode{np.argsort}.
\end{itemize}

\textbf{Solution:}
\begin{python}
import numpy as np

# Create p=100 values of alpha uniformly in [a,b]
p = 100
alpha_tilde = np.linspace(a, b, p)

# Create feature matrix Z for training data
Ztr = np.zeros((len(xtr), p))
for j in range(p):
    Ztr[:, j] = np.exp(-alpha_tilde[j] * xtr)

# Create feature matrix Z for test data
Zts = np.zeros((len(xts), p))
for j in range(p):
    Zts[:, j] = np.exp(-alpha_tilde[j] * xts)

# Fit LASSO model
model = Lasso(lam=lam)
beta_tilde = model.fit(Ztr, ytr)

# Predict on test data
yhat_ts = model.predict(Zts)

# Measure test error (MSE)
test_error = np.mean((yts - yhat_ts)**2)
print(f"Test error: {test_error}")

# Find coefficients corresponding to largest k=3 values
k = 3
largest_indices = np.argsort(np.abs(beta_tilde))[-k:]

# Extract the corresponding alpha and beta values
alpha_j = alpha_tilde[largest_indices]
beta_j = beta_tilde[largest_indices]

print(f"Selected alpha values: {alpha_j}")
print(f"Selected beta values: {beta_j}")
\end{python}


\item \emph{Minimizing an $\ell_1$ objective.}
In this problem, we will show how to minimize a simple scalar function with
an $\ell_1$-term.  Given $y$ and $\lambda > 0$, suppose we wish to find the minimum,
\[
    \widehat{w} = \argmin_w J(w) = \frac{1}{2}(y-w)^2 + \lambda|w|.
\]
Write $\widehat{w}$ in terms of $y$ and $\lambda$.  Since $|w|$ is not
differentiable everywhere, you cannot simple set $J'(w)=0$ and solve for $w$.
Instead, you have to look at three cases:
\begin{enumerate}[(i)]
  \item First, suppose there is a minima at $w > 0$.  In this region, $|w| = w$.
  Since the set $w > 0$ is open, at any minima $J'(w)=0$. Solve for $w$ and
  test if the solution indeed satisfies $w > 0$.
  \item Similarly, suppose $w < 0$. Solve for $J'(w) = 0$ and test if the solution
  satisfies the assumption that $w < 0$.
  \item If neither of the above cases have a minima, then the minima must be at
  $w=0$.
\end{enumerate}

\textbf{Solution:}

\textbf{Case (i): $w > 0$}
For $w > 0$, we have $|w| = w$, so:
\[
J(w) = \frac{1}{2}(y-w)^2 + \lambda w
\]
\[
J'(w) = -(y-w) + \lambda = -y + w + \lambda
\]
Setting $J'(w) = 0$:
\[
w = y - \lambda
\]
This solution is valid if $w > 0$, which means $y - \lambda > 0$, or $y > \lambda$.

\textbf{Case (ii): $w < 0$}
For $w < 0$, we have $|w| = -w$, so:
\[
J(w) = \frac{1}{2}(y-w)^2 + \lambda(-w) = \frac{1}{2}(y-w)^2 - \lambda w
\]
\[
J'(w) = -(y-w) - \lambda = -y + w - \lambda
\]
Setting $J'(w) = 0$:
\[
w = y + \lambda
\]
This solution is valid if $w < 0$, which means $y + \lambda < 0$, or $y < -\lambda$.

\textbf{Case (iii): $w = 0$}
If neither case (i) nor case (ii) applies, then the minimum is at $w = 0$. This occurs when $-\lambda \leq y \leq \lambda$.

\textbf{Final Solution:}
\[
\widehat{w} = \begin{cases}
y - \lambda & \text{if } y > \lambda \\
0 & \text{if } -\lambda \leq y \leq \lambda \\
y + \lambda & \text{if } y < -\lambda
\end{cases}
\]

This can also be written as the soft-thresholding function:
\[
\widehat{w} = \text{sign}(y) \max(0, |y| - \lambda)
\]

\end{enumerate}
\end{document}
